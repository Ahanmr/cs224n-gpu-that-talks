
General Implementation: 

- Speed up training, use preprocessed npy files with td.data pipeline
    * Implement dataset preprocessing
    * Implement tf.data.Dataset based training / validation pipeline
    - Implement bug fix: loss masking for padded batches (text2mel,ssrn)
    - Fix feedable iterator hack for validation data pipeline 
    - Implement bucketed batches 
- Implement direct transfer learning 
- Implement partial loading of ljspeech model 

- implement dropout, layernorm / other training tricks
- Document params file parameters
* Fix queue parameter tuning during training / edit dsp_utils, data_load to use only TF API and use tf.data pipeline
* Fix deprecated WARNING: switch to tf.train.MonitoredTrainingSession

Core Implementations:

* TextEnc implementation:
    * implement, test highway conv
    * implement, test text_enc block
    * test initialization of highway conv layers

* Decoder implementation:
    * implement, test causal conv
    * implement, test causal highway conv
    * implement, test causal decoder F2d 
    * implement, test causal decoder d2F 

* Attention implementation:
    * implement basic attention mechanism
    * add, test guided attention loss
    * test training with positional encoding

* SSRN implementation:
    * implement, test transposed convolution
    * implement full SSRN module
    * modify build graph
    * modify data pipeline for faster patch-based training 

* Test full data pipeline with kyubyong loaders + 230 training framework
    * predict_op, loss_op
    * train_op, train/predict on batch
    * training, checkpointing, evaluation framework
    * train/dev data load

* Inference/Synthesis graph 
    * implement restoring of variables 
    * implement output feedback

Experimentation:

- Indic
    - Rerun baseline training from scratch with tf.data pipeline
    - Fine-tune pretrained model from LJSpeech (hyperparam search)

- LJSpeech 
    * modify attention with position encodings, w w/o guided attention
    ^ multi-task learning, joint optimization
