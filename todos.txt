
General Implementation: 

- Data pipeline 
    * Implement dataset preprocessing
    * Implement tf.data.Dataset based training / validation pipeline
    * Fix deprecated WARNING: switch to tf.train.MonitoredTrainingSession
    - Fix feedable iterator hack for validation data pipeline 
    - Implement bucketed batches 
- Other improvements/bugfixes
    * Implement fix: loss masking for padded batches (text2mel,ssrn)
    * Implement stop prediction
    - Implement inference with stop prediction
    - Implement constrained attention at inference
    - Globally use only params.data_dir, remove wav, csv, other path specifications
    - implement dropout, layernorm / other training tricks
- For Transfer learning experiments    
    - Implement partial loading of ljspeech model 
    - Implement direct transfer learning 
- Documentation
    - Document params file parameters

Core Implementations:

* TextEnc implementation:
    * implement, test highway conv
    * implement, test text_enc block
    * test initialization of highway conv layers

* Decoder implementation:
    * implement, test causal conv
    * implement, test causal highway conv
    * implement, test causal decoder F2d 
    * implement, test causal decoder d2F 

* Attention implementation:
    * implement basic attention mechanism
    * add, test guided attention loss
    * test training with positional encoding

* SSRN implementation:
    * implement, test transposed convolution
    * implement full SSRN module
    * modify build graph
    * modify data pipeline for faster patch-based training 

* Test full data pipeline with kyubyong loaders + 230 training framework
    * predict_op, loss_op
    * train_op, train/predict on batch
    * training, checkpointing, evaluation framework
    * train/dev data load

* Inference/Synthesis graph 
    * implement restoring of variables 
    * implement output feedback

Experimentation:

- Indic
    - Rerun baseline training from scratch with tf.data pipeline, masked loss, stop prediction
    - Fine-tune pretrained model from LJSpeech (hyperparam search)

- LJSpeech 
    * modify attention with position encodings, w w/o guided attention
    ^ multi-task learning, joint optimization
